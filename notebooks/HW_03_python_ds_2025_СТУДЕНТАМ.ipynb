{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca0353b",
   "metadata": {
    "id": "aca0353b"
   },
   "source": [
    "# Домашнее задание 3. Парсинг, Git и тестирование на Python\n",
    "\n",
    "**Цели задания:**\n",
    "\n",
    "* Освоить базовые подходы к web-scraping с библиотеками `requests` и `BeautisulSoup`: навигация по страницам, извлечение HTML-элементов, парсинг.\n",
    "* Научиться автоматизировать задачи с использованием библиотеки `schedule`.\n",
    "* Попрактиковаться в использовании Git и оформлении проектов на GitHub.\n",
    "* Написать и запустить простые юнит-тесты с использованием `pytest`.\n",
    "\n",
    "\n",
    "В этом домашнем задании вы разработаете систему для автоматического сбора данных о книгах с сайта [Books to Scrape](http://books.toscrape.com). Нужно реализовать функции для парсинга всех страниц сайта, извлечения информации о книгах, автоматического ежедневного запуска задачи и сохранения результата.\n",
    "\n",
    "Важной частью задания станет оформление проекта: вы создадите репозиторий на GitHub, оформите `README.md`, добавите артефакты (код, данные, отчеты) и напишете базовые тесты на `pytest`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "K3JMV0qwmA_q",
   "metadata": {
    "id": "K3JMV0qwmA_q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q schedule pytest # установка библиотек, если ещё не"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "873d4904",
   "metadata": {
    "id": "873d4904"
   },
   "outputs": [],
   "source": [
    "# Библиотеки, которые могут вам понадобиться\n",
    "# При необходимости расширяйте список\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import schedule\n",
    "import json\n",
    "from typing import Tuple\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unTvsWaegHdj",
   "metadata": {
    "id": "unTvsWaegHdj"
   },
   "source": [
    "## Задание 1. Сбор данных об одной книге (20 баллов)\n",
    "\n",
    "В этом задании мы начнем подготовку скрипта для парсинга информации о книгах со страниц каталога сайта [Books to Scrape](https://books.toscrape.com/).\n",
    "\n",
    "Для начала реализуйте функцию `get_book_data`, которая будет получать данные о книге с одной страницы (например, с [этой](http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html)). Соберите всю информацию, включая название, цену, рейтинг, количество в наличии, описание и дополнительные характеристики из таблицы Product Information. Результат достаточно вернуть в виде словаря.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8** — помимо качественно написанного кода важно также документировать функции по стандарту:\n",
    "* кратко описать, что она делает и для чего нужна;\n",
    "* какие входные аргументы принимает, какого они типа и что означают по смыслу;\n",
    "* аналогично описать возвращаемые значения.\n",
    "\n",
    "*P. S. Состав, количество аргументов функции и тип возвращаемого значения можете менять как вам удобно. То, что написано ниже в шаблоне — лишь пример.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "UfD2vAjHkEoS",
   "metadata": {
    "id": "UfD2vAjHkEoS"
   },
   "outputs": [],
   "source": [
    "def get_response(url: str) -> Tuple[requests.Response | None, int | None]:\n",
    "    \"\"\"\n",
    "    Отправляет и получает ответ на GET запрос. Проверяет статусы ответов.\n",
    "\n",
    "    Args:\n",
    "        url: адрес страницы\n",
    "\n",
    "    Returns:\n",
    "        requests.Response в случае кода ответа 2хх\n",
    "            None во всех остальных случаях\n",
    "        Код HTTP ответа или None при requests.exceptions.RequestException\n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: Ошибка HTTP запроса\n",
    "            подобная ConnectionError, Timeout, HTTPError, SSLError итд.\n",
    "    \"\"\"\n",
    "    status_code = None\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if 200 <= response.status_code < 300:\n",
    "            return response, response.status_code\n",
    "        else:\n",
    "            status_code = response.status_code\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "\n",
    "    return None, status_code\n",
    "\n",
    "\n",
    "def get_book_data(book_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Собирает со страницы название книги, цену, рейтинг, количество в наличии,\n",
    "    описание, характеристики из таблицы Product Information.\n",
    "\n",
    "    Args:\n",
    "        book_url: ссылка на страницу с информацией о книге.\n",
    "\n",
    "    Returns:\n",
    "        словарь с характеристиками книги\n",
    "            title\n",
    "            price\n",
    "            availability\n",
    "            star_rating\n",
    "            description\n",
    "            upc\n",
    "            product_type\n",
    "            price_without_tax\n",
    "            price_with_tax\n",
    "            tax_rate\n",
    "            reviews_number\n",
    "    \"\"\"\n",
    "\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    book_dict = {}\n",
    "    response, status_code = get_response(book_url)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser', from_encoding='utf-8')\n",
    "        product_page = soup.find('article', attrs={'class': 'product_page'})\n",
    "        product_row = product_page.find('div', attrs={'class': 'row'})\n",
    "        # title\n",
    "        book_title = product_row.find('h1').text\n",
    "        book_dict['title'] = book_title\n",
    "        # price\n",
    "        price = product_row.find('p', attrs={'class': 'price_color'}).text\n",
    "        book_dict['price'] = price\n",
    "        # availability\n",
    "        availability = product_row.find('p', attrs={'class': 'instock availability'}).text.strip()\n",
    "        book_dict['availability'] = availability\n",
    "        # star-rating Four\n",
    "        star_rating = product_row.find('p', attrs={'class': 'star-rating'})\n",
    "        star_rating_class = star_rating.attrs['class']\n",
    "        book_dict['star_rating'] = len(star_rating_class) == 2 and star_rating_class[1]\n",
    "        # description\n",
    "        product_description = product_page.find('div', id=\"product_description\")\n",
    "        book_dict['description'] = product_description and product_description.find_next_sibling('p').text\n",
    "        # Product Information\n",
    "        product_information = product_page.find('table', attrs={'class': \"table table-striped\"})\n",
    "        table_ths = product_information.find_all('th')\n",
    "        for th in table_ths:\n",
    "            # upc\n",
    "            if th.string == 'UPC':\n",
    "                book_dict['upc'] = th.find_next_sibling('td').text\n",
    "            # product_type\n",
    "            elif th.string == 'Product Type':\n",
    "                book_dict['product_type'] = th.find_next_sibling('td').text\n",
    "            # price_without_tax\n",
    "            elif th.string == 'Price (excl. tax)':\n",
    "                book_dict['price_without_tax'] = th.find_next_sibling('td').text\n",
    "            # price_with_tax\n",
    "            elif th.string == 'Price (incl. tax)':\n",
    "                book_dict['price_with_tax'] = th.find_next_sibling('td').text\n",
    "            # tax_rate\n",
    "            elif th.string == 'Tax':\n",
    "                book_dict['tax_rate'] = th.find_next_sibling('td').text\n",
    "            # reviews_number\n",
    "            elif th.string == 'Number of reviews':\n",
    "                book_dict['reviews_number'] = th.find_next_sibling('td').text\n",
    "    else:\n",
    "        print(f\"Ошибка ответа от сервера. Код HTTP ответа: {status_code}\")\n",
    "\n",
    "    return book_dict\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "moRSO9Itp1LT",
   "metadata": {
    "id": "moRSO9Itp1LT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Story of Art\n"
     ]
    }
   ],
   "source": [
    "# Используйте для самопроверки\n",
    "book_url = 'https://books.toscrape.com/catalogue/the-story-of-art_500/index.html'\n",
    "book_dict = get_book_data(book_url)\n",
    "print(book_dict['title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u601Q4evosq6",
   "metadata": {
    "id": "u601Q4evosq6"
   },
   "source": [
    "## Задание 2. Сбор данных обо всех книгах (20 баллов)\n",
    "\n",
    "Создайте функцию `scrape_books`, которая будет проходиться по всем страницам из каталога (вида `http://books.toscrape.com/catalogue/page-{N}.html`) и осуществлять парсинг всех страниц в цикле, используя ранее написанную `get_book_data`.\n",
    "\n",
    "Добавьте аргумент-флаг, который будет отвечать за сохранение результата в файл: если он будет равен `True`, то информация сохранится в ту же папку в файл `books_data.txt`; иначе шаг сохранения будет пропущен.\n",
    "\n",
    "**Также не забывайте про соблюдение PEP-8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "kk78l6oDkdxl",
   "metadata": {
    "id": "kk78l6oDkdxl"
   },
   "outputs": [],
   "source": [
    "BASE_CATALOGUE_URL = \"https://books.toscrape.com/catalogue/\"\n",
    "BASE_CATALOGUE_PAGE_URL = \"https://books.toscrape.com/catalogue/page-{N}.html\"\n",
    "\n",
    "\n",
    "def get_books_links(url: str) -> list:\n",
    "    \"\"\"\n",
    "    Собирает со страницы каталога ссылки на страницы с книгами.\n",
    "\n",
    "    Args:\n",
    "        url: ссылка на страницу каталога.\n",
    "\n",
    "    Returns:\n",
    "        Список ссылок на страницы с книгами\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    response, status_code = get_response(url)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser', from_encoding='utf-8')\n",
    "        ordered_list = soup.find('ol', attrs={'class': 'row'})\n",
    "        h3_tags = ordered_list.find_all('h3')\n",
    "        for h3 in h3_tags:\n",
    "            links.append(h3.find('a')['href'])\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def get_catalogue_pages(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Собирает страницы каталога в словарь.\n",
    "\n",
    "    Args:\n",
    "        url: ссылка шаблон страницы каталога вида\n",
    "            https://books.toscrape.com/catalogue/page-{N}.html\n",
    "\n",
    "    Returns:\n",
    "        Словарь со ссылками на все страницы каталога\n",
    "    \"\"\"\n",
    "    catalogue_pages = {}\n",
    "    page_num = 1\n",
    "    while links := get_books_links(url.format(N=page_num)):\n",
    "        catalogue_pages[page_num] = links\n",
    "        page_num += 1\n",
    "\n",
    "    return catalogue_pages\n",
    "\n",
    "\n",
    "def runtime(func):\n",
    "    \"\"\"\n",
    "    Декоратор для замера времени выполнения парсинга.\n",
    "    \"\"\"\n",
    "    def runtime_wrapper(*args, **kwargs):\n",
    "        start_datetime = datetime.datetime.now()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_datetime = datetime.datetime.now()\n",
    "        time_difference = end_datetime - start_datetime\n",
    "        total_seconds = time_difference.total_seconds()\n",
    "        hours, remainder = divmod(total_seconds, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        print(f\"Время работы: {int(hours):02}:{int(minutes):02}:{seconds:05.2f}\")\n",
    "        return result\n",
    "\n",
    "    return runtime_wrapper\n",
    "\n",
    "\n",
    "@runtime\n",
    "def scrape_books(is_save: bool = True) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Собирает информацию о книгах на страницах каталога и в зависимости от\n",
    "    настройки выводит результаты в файл, и возвращает как результат работы\n",
    "\n",
    "    Args:\n",
    "        is_save: аргумент-флаг, который отвечает за сохранение результата\n",
    "            в файл, если он будет равен `True`, то информация сохраняется\n",
    "            построчно с конвертацией словаря в json\n",
    "            в ту же папку в файл `books_data.txt`.\n",
    "\n",
    "    Returns:\n",
    "        Список ссылок на страницы с книгами\n",
    "    \"\"\"\n",
    "\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    books = []\n",
    "\n",
    "    for page, links in get_catalogue_pages(BASE_CATALOGUE_PAGE_URL).items():\n",
    "        for link in links:\n",
    "            book_dict = get_book_data(BASE_CATALOGUE_URL + link)\n",
    "            books.append(book_dict)\n",
    "\n",
    "    if is_save:\n",
    "        with open('artifacts/books_data.txt', 'w', encoding='UTF-8') as file:\n",
    "            for book in books:\n",
    "                file.write(json.dumps(book, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    return books\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "Bt7mrXcbkj5Q",
   "metadata": {
    "id": "Bt7mrXcbkj5Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время работы: 00:09:55.84\n",
      "<class 'list'> 1000\n"
     ]
    }
   ],
   "source": [
    "# Проверка работоспособности функции\n",
    "res = scrape_books(is_save=True) # Допишите ваши аргументы\n",
    "print(type(res), len(res)) # и проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5fd728nl8a8",
   "metadata": {
    "id": "z5fd728nl8a8"
   },
   "source": [
    "## Задание 3. Настройка регулярной выгрузки (10 баллов)\n",
    "\n",
    "Настройте автоматический запуск функции сбора данных каждый день в 19:00.\n",
    "Для автоматизации используйте библиотеку `schedule`. Функция должна запускаться в указанное время и сохранять обновленные данные в текстовый файл.\n",
    "\n",
    "\n",
    "\n",
    "Бесконечный цикл должен обеспечивать постоянное ожидание времени для запуска задачи и выполнять ее по расписанию. Однако чтобы не перегружать систему, стоит подумать о том, чтобы выполнять проверку нужного времени не постоянно, а раз в какой-то промежуток. В этом вам может помочь `time.sleep(...)`.\n",
    "\n",
    "Проверьте работоспособность кода локально на любом времени чч:мм.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "SajRRCj4n8BZ",
   "metadata": {
    "id": "SajRRCj4n8BZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Парсинг стартовал в 2025-11-09 12:04:08\n",
      "Время работы: 00:09:49.52\n"
     ]
    }
   ],
   "source": [
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "def scrape_job():\n",
    "    print(f\"Парсинг стартовал в {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\")\n",
    "    scrape_books()\n",
    "\n",
    "\n",
    "schedule.every().day.at(\"19:00\").do(scrape_job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(30)\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XFiPtEyaoLxq",
   "metadata": {
    "id": "XFiPtEyaoLxq"
   },
   "source": [
    "## Задание 4. Написание автотестов (15 баллов)\n",
    "\n",
    "Создайте минимум три автотеста для ключевых функций парсинга — например, `get_book_data` и `scrape_books`. Идеи проверок (можете использовать свои):\n",
    "\n",
    "* данные о книге возвращаются в виде словаря с нужными ключами;\n",
    "* список ссылок или количество собранных книг соответствует ожиданиям;\n",
    "* значения отдельных полей (например, `title`) корректны.\n",
    "\n",
    "Оформите тесты в отдельном скрипте `tests/test_scraper.py`, используйте библиотеку `pytest`. Убедитесь, что тесты проходят успешно при запуске из терминала командой `pytest`.\n",
    "\n",
    "Также выведите результат их выполнения в ячейке ниже.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "lBFAw4b3z8QY",
   "metadata": {
    "id": "lBFAw4b3z8QY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]9;4;3;\u001b\\\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.11, pytest-9.0.0, pluggy-1.6.0\n",
      "rootdir: /home/george/projects/python/HW-03-VSC\n",
      "plugins: anyio-4.11.0\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_scraper.py \u001b]9;4;1;0\u001b\\\u001b[32m.\u001b[0m\u001b]9;4;1;20\u001b\\\u001b[32m.\u001b[0m\u001b]9;4;1;40\u001b\\\u001b[32m.\u001b[0m\u001b]9;4;1;60\u001b\\\u001b[32m.\u001b[0m\u001b]9;4;1;80\u001b\\\u001b[32m.\u001b[0m\u001b[32m                                              [100%]\u001b[0m\u001b]9;4;0;\u001b\\\n",
      "\n",
      "\u001b[32m======================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 604.17s (0:10:04)\u001b[0m\u001b[32m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Ячейка для демонстрации работоспособности\n",
    "# Сам код напишите в отдельном скрипте\n",
    "! pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cRSQlHfRtOdN",
   "metadata": {
    "id": "cRSQlHfRtOdN"
   },
   "source": [
    "## Задание 5. Оформление проекта на GitHub и работа с Git (35 баллов)\n",
    "\n",
    "В этом задании нужно воспользоваться системой контроля версий Git и платформой GitHub для хранения и управления своим проектом. **Ссылку на свой репозиторий пришлите в форме для сдачи ответа.**\n",
    "\n",
    "### Пошаговая инструкция и задания\n",
    "\n",
    "**1. Установите Git на свой компьютер.**\n",
    "\n",
    "* Для Windows: [скачайте установщик](https://git-scm.com/downloads) и выполните установку.\n",
    "* Для macOS:\n",
    "\n",
    "  ```\n",
    "  brew install git\n",
    "  ```\n",
    "* Для Linux:\n",
    "\n",
    "  ```\n",
    "  sudo apt update\n",
    "  sudo apt install git\n",
    "  ```\n",
    "\n",
    "**2. Настройте имя пользователя и email.**\n",
    "\n",
    "Это нужно для подписи ваших коммитов, сделайте в терминале через `git config ...`.\n",
    "\n",
    "**3. Создайте аккаунт на GitHub**, если у вас его еще нет:\n",
    "[https://github.com](https://github.com)\n",
    "\n",
    "**4. Создайте новый репозиторий на GitHub:**\n",
    "\n",
    "* Найдите кнопку **New repository**.\n",
    "* Укажите название, краткое описание, выберите тип **Public** (чтобы мы могли проверить ДЗ).\n",
    "* Не ставьте галочку Initialize this repository with a README.\n",
    "\n",
    "**5. Создайте локальную папку с проектом.** Можно в терминале, можно через UI, это не имеет значения.\n",
    "\n",
    "**6. Инициализируйте Git в этой папке.** Здесь уже придется воспользоваться некоторой командой в терминале.\n",
    "\n",
    "**7. Привяжите локальный репозиторий к удаленному на GitHub.**\n",
    "\n",
    "**8. Создайте ветку разработки.** По умолчанию вы будете находиться в ветке `main`, создайте и переключитесь на ветку `hw-books-parser`.\n",
    "\n",
    "**9. Добавьте в проект следующие файлы и папки:**\n",
    "\n",
    "* `scraper.py` — ваш основной скрипт для сбора данных.\n",
    "* `README.md` — файл с кратким описанием проекта:\n",
    "\n",
    "  * цель;\n",
    "  * инструкции по запуску;\n",
    "  * список используемых библиотек.\n",
    "* `requirements.txt` — файл со списком зависимостей, необходимых для проекта (не присылайте все из глобального окружения, создайте изолированную виртуальную среду, добавьте в нее все нужное для проекта и получите список библиотек через `pip freeze`).\n",
    "* `artifacts/` — папка с результатами парсинга (`books_data.txt` — полностью или его часть, если весь не поместится на GitHub).\n",
    "* `notebooks/` — папка с заполненным ноутбуком `HW_03_python_ds_2025.ipynb` и запущенными ячейками с выводами на экран.\n",
    "* `tests/` — папка с тестами на `pytest`, оформите их в формате скрипта(-ов) с расширением `.py`.\n",
    "* `.gitignore` — стандартный файл, который позволит исключить временные файлы при добавлении в отслеживаемые (например, `__pycache__/`, `.DS_Store`, `*.pyc`, `venv/` и др.).\n",
    "\n",
    "\n",
    "**10. Сделайте коммит.**\n",
    "\n",
    "**11. Отправьте свою ветку на GitHub.**\n",
    "\n",
    "**12. Создайте Pull Request:**\n",
    "\n",
    "* Перейдите в репозиторий на GitHub.\n",
    "* Нажмите кнопку **Compare & pull request**.\n",
    "* Укажите, что было добавлено, и нажмите **Create pull request**.\n",
    "\n",
    "**13. Выполните слияние Pull Request:**\n",
    "\n",
    "* Убедитесь, что нет конфликтов.\n",
    "* Нажмите **Merge pull request**, затем **Confirm merge**.\n",
    "\n",
    "**14. Скачайте изменения из основной ветки локально.**\n",
    "\n",
    "\n",
    "\n",
    "### Требования к итоговому репозиторию\n",
    "\n",
    "* Файл `scraper.py` с рабочим кодом парсера.\n",
    "* `README.md` с описанием проекта и инструкцией по запуску.\n",
    "* Папка `artifacts/` с результатом сбора данных (`.txt` файл).\n",
    "* Папка `tests/` с тестами на `pytest`.\n",
    "* Папка `notebooks/` с заполненным ноутбуком `HW_03_python_ds_2025.ipynb`.\n",
    "* Pull Request с комментарием из ветки `hw-books-parser` в ветку `main`.\n",
    "* Примерная структура:\n",
    "\n",
    "  ```\n",
    "  books_scraper/\n",
    "  ├── artifacts/\n",
    "  │   └── books_data.txt\n",
    "  ├── notebooks/\n",
    "  │   └── HW_03_python_ds_2025.ipynb\n",
    "  ├── scraper.py\n",
    "  ├── README.md\n",
    "  ├── tests/\n",
    "  │   └── test_scraper.py\n",
    "  ├── .gitignore\n",
    "  └── requirements.txt\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
